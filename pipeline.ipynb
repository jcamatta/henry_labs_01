{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442d897c",
   "metadata": {},
   "source": [
    "Que pasa si se ingresa una nueva pelicula o un conjunto de nuevas peliculas?\n",
    "\n",
    "Este notebook esta dedicado a eso, al proceso de actualizar nuestra base de datos, realizar las transformaciones necesarias e implementarlas en nuestro modelo de recomendacion.\n",
    "\n",
    "Esto es, la creacion de un PIPELINE. Una secuencia de transformaciones realizadas a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a9819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af1386",
   "metadata": {},
   "source": [
    "## Cargamos los nuevos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbbb2274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usaremos data previa para \"recrear\" el caso\n",
    "data = pd.read_csv('../data/movies_dataset.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e965cd",
   "metadata": {},
   "source": [
    "Se asume que los datos vienen en un formato de dato, esto es:\n",
    "\n",
    "1. El nombre de las columnas\n",
    "2. Los tipos de datos en cada columna\n",
    "\n",
    "Por lo que previamente se le debe realizar una mini-transformacion en caso de que sea necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d488b55b",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db4c1102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(data):\n",
    "    # Si se ejecuta cada paso correctamente, entonces retorna True\n",
    "    \n",
    "    # Primera etapa\n",
    "    output = clean_data(data)\n",
    "    \n",
    "    # Segunda etapa\n",
    "    output = feature_engineer(output)\n",
    "    \n",
    "    # La linea de abajo por ahora normaliza valores numericos, puesto que esto lo realiza ya el modelo, no es necesario.\n",
    "    # output = normalize_data(output)\n",
    "    \n",
    "    # Etapa final\n",
    "    return update_db(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b2d6f4",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d7863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(value):\n",
    "    ''' extrae de los valores anidados, el valor del campo \"name\" ''' \n",
    "    output = []\n",
    "    try:\n",
    "        value = ast.literal_eval(value)\n",
    "        if isinstance(value, list):\n",
    "            for dictionary in value:\n",
    "                output.append(dictionary['name'])\n",
    "        elif isinstance(value, dict):\n",
    "            output.append(value['name'])\n",
    "    finally:\n",
    "        for i in range(len(output)):\n",
    "            output[i] = output[i].lower()\n",
    "        return output\n",
    "    \n",
    "def extract_iso(value):\n",
    "    ''' extrae de los valores anidados, el valor del campo \"iso\" ''' \n",
    "    output = []\n",
    "    try:\n",
    "        value = ast.literal_eval(value)\n",
    "        if isinstance(value, list):\n",
    "            for dictionary in value:\n",
    "                for key, value in dictionary.items():\n",
    "                    if 'iso' in key:\n",
    "                        output.append(value)\n",
    "    finally:\n",
    "        for i in range(len(output)):\n",
    "            output[i] = output[i].lower()\n",
    "        return output\n",
    "    \n",
    "def parse_zero(v):\n",
    "    try:\n",
    "        return float(v)\n",
    "    except ValueError as e:\n",
    "        return 0\n",
    "    \n",
    "def clean_date(value):\n",
    "    try:\n",
    "        year, month, day = value.split('-')\n",
    "        x, y, z = int(year), int(month), int(day)\n",
    "        if len(year) == 4:\n",
    "            if len(month) == 2:\n",
    "                if len(day) == 2:\n",
    "                    return value\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1de6c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \"\"\"En esta etapa: eliminamos columnas y duplicados, extraemos valores e inputamos datos faltantes\"\"\"\n",
    "    \n",
    "    # Seleccionamos las columnas a utilizar.\n",
    "    columnas_a_usar = ['belongs_to_collection', 'budget', 'genres', 'overview', 'popularity', 'production_companies',\n",
    "                       'release_date', 'revenue', 'runtime', 'production_countries', 'spoken_languages', \n",
    "                       'title', 'vote_average']\n",
    "    new_data = data[columnas_a_usar].copy()\n",
    "    \n",
    "    # Desanidamos los datos.\n",
    "    \n",
    "    # Extraemos el valor del key \"name\" de las columnas belongs, genres, production_companies\n",
    "    dstructure_cols = ['belongs_to_collection', 'genres', 'production_companies',\n",
    "                       'production_countries', 'spoken_languages']\n",
    "    for c in dstructure_cols:\n",
    "        new_data[c] = data[c].apply(extract_name)\n",
    "        if c in ['production_countries', 'spoken_languages']:\n",
    "            new_data[c + '_iso'] = data[c].apply(extract_iso)\n",
    "        \n",
    "    # En budget, popularity, revenue, runtime, vote_average\n",
    "    cols = ['budget', 'popularity', 'revenue', 'runtime', 'vote_average']\n",
    "    for c in cols:\n",
    "        new_data[c] = new_data[c].apply(parse_zero) # Remplazamos por 0 los valores no-numericos\n",
    "        new_data[c] = new_data[c].apply(lambda v: 0 if v < 0 else v) # Remplazamos por 0 los valores negativos\n",
    "        new_data[c] = new_data[c].fillna(0) # Remplazamos por 0 los valores faltantes \n",
    "    \n",
    "    # Convertimos a NA cualquier valor en release_date que tenga el formato YYYY-mm-dd\n",
    "    new_data['release_date'] = new_data['release_date'].apply(clean_date)\n",
    "    \n",
    "    # Eliminamos las filas con NA en el campo release_date\n",
    "    new_data.dropna(subset=['release_date'], axis=0, inplace=True)\n",
    "    \n",
    "    # Actualizamos el data type de release_date\n",
    "    new_data['release_date'] = pd.to_datetime(new_data['release_date'])\n",
    "    \n",
    "    # Convertimos a lower case los titulos\n",
    "    new_data['title'] = new_data['title'].str.lower()\n",
    "    \n",
    "    # Eliminamos duplicados (consideramos duplicados aquellas peliculas con el mismo release_date)\n",
    "    new_data.drop_duplicates(subset=['title', 'release_date'], inplace=True)\n",
    "    \n",
    "    return new_data    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22995d15",
   "metadata": {},
   "source": [
    "<h2> Feature Engineer </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75c0a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la funcion para crear la columna era en object_columns\n",
    "def create_era(year):\n",
    "    if isinstance(year, int):\n",
    "        if year < 1960:\n",
    "            return 'clasicas'\n",
    "        elif year < 2000:\n",
    "            return 'retro'\n",
    "        elif year < 2021:\n",
    "            return 'contemporaneas'\n",
    "    return ''\n",
    "\n",
    "# Creamos la funcion para crear la columna ranking en object_columns\n",
    "def create_ranking(vote):\n",
    "    if vote < 3: # si esta entre (0, 3)\n",
    "        return 'mala'\n",
    "    elif vote < 6: # si esta entre (3, 5)\n",
    "        return 'regular'\n",
    "    elif vote < 7.5: # si esta entre (5, 7.5)\n",
    "        return 'buena'\n",
    "    elif vote < 8.5: # si esta entre (7, 8.5)\n",
    "        return 'muy buena'\n",
    "    else: # si es mayor a 8.5\n",
    "        return 'excelente'\n",
    "    \n",
    "# estos criterios son arbitrarios y se pueden probar otros si se quiere.\n",
    "\n",
    "def limpiar_strings(inlist):\n",
    "    outlist = []\n",
    "    for string in inlist:\n",
    "        # dada una string eliminamos cualquier caracter segun el regex\n",
    "        x = re.sub(r'[^a-zA-Z0-9\\s]', '', string)\n",
    "        x = ' '.join(x.split())\n",
    "        outlist.append(x)\n",
    "    return outlist\n",
    "\n",
    "def get_keyword(text, nlp, stopwords):\n",
    "    '''retorna una lista con palabras claves de \"text\"'''\n",
    "    if not isinstance(text, str):\n",
    "        return np.nan\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    output = []\n",
    "    for token in doc.noun_chunks:\n",
    "        words = token.lemma_.lower().split(' ')\n",
    "        string = []\n",
    "        for word in words:\n",
    "            # eliminamos del token todos los stopwords\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            string.append(word)\n",
    "        if len(string) >= 2: # nos quedamos con los \"tokens\" con mas de 2 palabras\n",
    "            output.append(' '.join(string))\n",
    "            \n",
    "    return limpiar_strings(output)\n",
    "    \n",
    "    \n",
    "def create_doc_vocab(df, text_column=None):\n",
    "    ''' Crea la columna DOC y retorna Vocab '''\n",
    "    \n",
    "    # Si text_column es None, entonces usa todas las columnas no-numericas excepto title.\n",
    "    if not text_column:\n",
    "        text_column = df.select_dtypes(exclude=np.number).drop(columns=['title']).columns.tolist()\n",
    "\n",
    "    # Crea la columna doc, vacia por el momento.\n",
    "    df['doc'] = ''\n",
    "    \n",
    "    # Crea vocab\n",
    "    vocab = []\n",
    "    \n",
    "    \n",
    "    soup = []\n",
    "    # Este codigo recorre cada fila, para cada fila hace uso de todas las columnas text_column\n",
    "    # si una palabra en una columna \"C\" no se encuentra en vocab, lo agrega.\n",
    "    # soup es una lista de tokens asociada a cada pelicula/fila\n",
    "    for i, row in df.iterrows():\n",
    "        for c in text_column:\n",
    "            if isinstance(row[c], list):\n",
    "                for token in row[c]:\n",
    "                    if token not in vocab:\n",
    "                        vocab.append(token)\n",
    "                    soup.append(token)\n",
    "            elif isinstance(row[c], str):\n",
    "                if row[c] not in vocab:\n",
    "                    vocab.append(row[c])\n",
    "                soup.append(row[c])\n",
    "        df.loc[i, 'doc'] = ' '.join(soup)\n",
    "        soup = []\n",
    "        \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6854176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(data):\n",
    "    # Creamos el objecto nlp\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    stopwords = nlp.Defaults.stop_words # stops words basado en \"en_core_web_sm\"\n",
    "    \n",
    "    # Creamos una copia de data\n",
    "    new_data = data.copy()\n",
    "    \n",
    "    # Creamos la columna return y remplazamos los valores nan por 0\n",
    "    new_data['return'] = new_data['revenue'] / new_data['budget']\n",
    "    \n",
    "    # Remplazamos valores con inf o -inf por np.nan\n",
    "    new_data['return'] = new_data['return'].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Remplazamos los valores nan por 0\n",
    "    new_data['return'] = new_data['return'].fillna(0)\n",
    "    \n",
    "    # Creamos un diccionario para traducir los meses\n",
    "    meses = {'october': 'octubre', 'december': 'diciembre', 'february': 'febrero', 'november': 'noviembre', \n",
    "             'september': 'septiembre', 'may': 'mayo', 'april': 'abril', 'august': 'agosto', 'july': 'julio',\n",
    "             'june': 'junio', 'january': 'enero','march': 'marzo'}\n",
    "\n",
    "    # Creamos un diccionario para traducir los dias de la semana\n",
    "    dias = {'monday': 'lunes','friday': 'viernes', 'thursday': 'martes', 'wednesday': 'miercoles',\n",
    "            'saturday': 'sabado', 'tuesday': 'jueves', 'sunday': 'domingo'}\n",
    "\n",
    "    # Creamos la columna release-year\n",
    "    new_data['release_year'] = new_data.release_date.dt.year\n",
    "    \n",
    "    # Creamos la columna release_month\n",
    "    new_data['release_month'] = new_data.release_date.dt.month_name().str.lower().replace(meses)\n",
    "    \n",
    "    # Creamos la columna release_day\n",
    "    new_data['release_day'] = new_data.release_date.dt.day_name().str.lower().replace(dias)\n",
    "    \n",
    "    # Creamos la columna era\n",
    "    new_data['era'] = new_data['release_year'].apply(create_era)\n",
    "    \n",
    "    # Creamos la columna ranking\n",
    "    new_data['ranking'] = new_data['vote_average'].apply(create_ranking)\n",
    "    \n",
    "    # Creamos la columna overview_keywords\n",
    "    new_data['overview_keywords'] = new_data['overview'].apply(lambda v: get_keyword(v, nlp, stopwords))\n",
    "    \n",
    "    # Creamos la columna doc\n",
    "    columnas_para_doc = ['belongs_to_collection', 'genres', 'production_companies', \n",
    "                         'production_countries_iso', 'spoken_languages_iso', 'era', 'ranking', 'overview_keywords']\n",
    "    new_vocab = create_doc_vocab(new_data, columnas_para_doc)\n",
    "    \n",
    "    # Actualizamos vocab\n",
    "    \n",
    "    # Convertimos la lista a Series object\n",
    "    updated_vocab = pd.Series(new_vocab)\n",
    "    \n",
    "    # Importamos el anterior vocab\n",
    "    try:\n",
    "        latest_vocab = pd.read_csv('../data/latest_vocab.csv')\n",
    "        updated_vocab = pd.concat([latest_vocab, new_vocab]).drop_duplicates()\n",
    "    except FileNotFoundError:\n",
    "        print('latest_vocab.csv no existia y fue creado')\n",
    "    finally:\n",
    "        # Actualizamos el vocab y lo almacenamos\n",
    "        updated_vocab.to_csv('../data/latest_vocab.csv')\n",
    "    \n",
    "    # Dropeamos las columnas que no vamos a utilizar mas\n",
    "    columnas_a_usar = ['title', 'doc', 'popularity', 'runtime', 'return', 'budget', 'revenue', \n",
    "                       'release_year', 'release_month', 'release_day', \n",
    "                       'production_countries', 'production_companies', 'genres', 'spoken_languages']\n",
    "    new_data = new_data[columnas_a_usar]\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5817bc1a",
   "metadata": {},
   "source": [
    "## Normalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b097c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    \n",
    "    # Normalizamos los valores numericos.\n",
    "    \n",
    "    # Separamos el dataframe entre valores numericos y no-numericos\n",
    "    numeric_df = data.select_dtypes(include=np.number)\n",
    "    object_df = data.select_dtypes(exclude=np.number)\n",
    "    \n",
    "    # Creamos el objeto de normalizacion\n",
    "    rscaler = RobustScaler()\n",
    "    \n",
    "    # Normalizamos los datos\n",
    "    numeric_df = rscaler.fit_transform(numeric_df)\n",
    "    \n",
    "    return pd.concat([object_df, numeric_df], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ccfb28",
   "metadata": {},
   "source": [
    "## Actualizar base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f18a1ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db(new_data):\n",
    "    updated = new_data\n",
    "    try:\n",
    "        latest = pd.read_csv('../data/latest_movies.csv')\n",
    "        updated = pd.concat([latest, new_data], axis=0)\n",
    "        updated = updated.drop_duplicates(keep='first')\n",
    "    except FileNotFoundError:\n",
    "           print('El archivo latest_movies.csv no existia y fue creado')\n",
    "    finally:\n",
    "        updated.to_csv('../data/latest.csv', mode='w', header=True, index=False)\n",
    "        return updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cf65377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>...</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 10194, 'name': 'Toy Story Collection', ...</td>\n",
       "      <td>30000000</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
       "      <td>http://toystory.disney.com/toy-story</td>\n",
       "      <td>862</td>\n",
       "      <td>tt0114709</td>\n",
       "      <td>en</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-10-30</td>\n",
       "      <td>373554033.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>False</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000000</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8844</td>\n",
       "      <td>tt0113497</td>\n",
       "      <td>en</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-15</td>\n",
       "      <td>262797249.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>False</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adult                              belongs_to_collection    budget   \n",
       "0  False  {'id': 10194, 'name': 'Toy Story Collection', ...  30000000  \\\n",
       "1  False                                                NaN  65000000   \n",
       "\n",
       "                                              genres   \n",
       "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...  \\\n",
       "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
       "\n",
       "                               homepage    id    imdb_id original_language   \n",
       "0  http://toystory.disney.com/toy-story   862  tt0114709                en  \\\n",
       "1                                   NaN  8844  tt0113497                en   \n",
       "\n",
       "  original_title                                           overview  ...   \n",
       "0      Toy Story  Led by Woody, Andy's toys live happily in his ...  ...  \\\n",
       "1        Jumanji  When siblings Judy and Peter discover an encha...  ...   \n",
       "\n",
       "  release_date      revenue runtime   \n",
       "0   1995-10-30  373554033.0    81.0  \\\n",
       "1   1995-12-15  262797249.0   104.0   \n",
       "\n",
       "                                    spoken_languages    status   \n",
       "0           [{'iso_639_1': 'en', 'name': 'English'}]  Released  \\\n",
       "1  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...  Released   \n",
       "\n",
       "                                     tagline      title  video vote_average   \n",
       "0                                        NaN  Toy Story  False          7.7  \\\n",
       "1  Roll the dice and unleash the excitement!    Jumanji  False          6.9   \n",
       "\n",
       "  vote_count  \n",
       "0     5415.0  \n",
       "1     2413.0  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Como venian los datos:\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70013365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest_vocab.csv no existia y fue creado\n",
      "El archivo latest_movies.csv no existia y fue creado\n"
     ]
    }
   ],
   "source": [
    "# Como quedaron:\n",
    "start = time.time()\n",
    "new = pipeline(data)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78cfda19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El proceso demoro 13.1\n"
     ]
    }
   ],
   "source": [
    "print(f'El proceso demoro {(end - start) / 60:.1f} minutos')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
